{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bae8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "\n",
    "    df = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae880c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_target(df, threshold=6):\n",
    "\n",
    "    df['good_wine'] = (df['quality'] >= threshold).astype(int)\n",
    "\n",
    "    print(f\"\\nBinary target created:\")\n",
    "    print(f\"  Good wines (quality >= {threshold}): {df['good_wine'].sum()}\")\n",
    "    print(f\"  Not good wines (quality < {threshold}): {(df['good_wine'] == 0).sum()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ef7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_size, val_size, test_size):\n",
    "    feature_cols = [col for col in df.columns if col not in ['quality', 'good_wine']]\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['good_wine'].values\n",
    "    \n",
    "    X_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    \n",
    "    X_val = X[train_size:train_size + val_size]\n",
    "    y_val = y[train_size:train_size + val_size]\n",
    "    \n",
    "    X_test = X[train_size + val_size:train_size + val_size + test_size]\n",
    "    y_test = y[train_size + val_size:train_size + val_size + test_size]\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(X_train, X_val, X_test):\n",
    "   \n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    std[std == 0] = 1\n",
    " \n",
    "    X_train_norm = (X_train - mean) / std\n",
    "    X_val_norm = (X_val - mean) / std\n",
    "    X_test_norm = (X_test - mean) / std\n",
    "    \n",
    "    print(f\"\\nZ-score normalization applied using training set statistics.\")\n",
    "    \n",
    "    return X_train_norm, X_val_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efbdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_knn(X_train, y_train, X_val, y_val, k_values):\n",
    "\n",
    "    validation_accuracies = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    print(f\"\\nTraining k-NN classifiers for k = 1 to {max(k_values)}...\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = knn.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        error = 1 - accuracy\n",
    "        \n",
    "        validation_accuracies.append(accuracy)\n",
    "        validation_errors.append(error)\n",
    "\n",
    "    best_idx = np.argmax(validation_accuracies)\n",
    "    best_k = k_values[best_idx]\n",
    "    best_accuracy = validation_accuracies[best_idx]\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  Best k: {best_k}\")\n",
    "    print(f\"  Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"  Best validation error: {1 - best_accuracy:.4f}\")\n",
    "    \n",
    "    return best_k, best_accuracy, validation_accuracies, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e472594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(X_train, y_train, X_test, y_test, best_k):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_error = 1 - test_accuracy\n",
    "    \n",
    "    print(f\"\\nTest Set Results (k={best_k}):\")\n",
    "    print(f\"  Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Generalisation error: {test_error:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Not Good', 'Good']))\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return test_accuracy, test_error, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curve(k_values, validation_errors, title, filename):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(k_values, validation_errors, 'b-', linewidth=1.5, marker='o', \n",
    "             markersize=3, label='Validation Error')\n",
    "    \n",
    "    best_idx = np.argmin(validation_errors)\n",
    "    best_k = k_values[best_idx]\n",
    "    best_error = validation_errors[best_idx]\n",
    "    plt.scatter([best_k], [best_error], color='red', s=150, zorder=5, \n",
    "                label=f'Best k={best_k} (Error={best_error:.4f})')\n",
    "    \n",
    "    plt.xlabel('k (Number of Neighbours)', fontsize=12)\n",
    "    plt.ylabel('Validation Error', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(0, max(k_values) + 1, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"\\nPlot saved as '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, train_size, val_size, test_size, experiment_name):\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"EXPERIMENT: {experiment_name}\")\n",
    "    print(f\"Split: Train={train_size}, Validation={val_size}, Test={test_size}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = split_data(\n",
    "        df, train_size, val_size, test_size\n",
    "    )\n",
    "    \n",
    "    X_train_norm, X_val_norm, X_test_norm = zscore_normalize(X_train, X_val, X_test)\n",
    "    \n",
    "    k_values = list(range(1, 101))\n",
    "    best_k, best_val_acc, val_accuracies, val_errors = train_and_evaluate_knn(\n",
    "        X_train_norm, y_train, X_val_norm, y_val, k_values\n",
    "    )\n",
    "    \n",
    "    test_acc, test_error, classifier = evaluate_on_test(\n",
    "        X_train_norm, y_train, X_test_norm, y_test, best_k\n",
    "    )\n",
    "    \n",
    "    plot_filename = f'validation_curve_{experiment_name.lower().replace(\" \", \"_\")}.png'\n",
    "    plot_validation_curve(\n",
    "        k_values, val_errors,\n",
    "        f'k-NN Validation Error vs k ({experiment_name})',\n",
    "        plot_filename\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'train_size': train_size,\n",
    "        'val_size': val_size,\n",
    "        'test_size': test_size,\n",
    "        'best_k': best_k,\n",
    "        'best_validation_accuracy': best_val_acc,\n",
    "        'validation_error': 1 - best_val_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'generalisation_error': test_error,\n",
    "        'validation_errors': val_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filepath = 'sparklingwine.csv'\n",
    "    df = load_data(filepath)\n",
    "    \n",
    "    df = create_binary_target(df, threshold=6)\n",
    "\n",
    "    results1 = run_experiment(\n",
    "        df, \n",
    "        train_size=900, \n",
    "        val_size=300, \n",
    "        test_size=400,\n",
    "        experiment_name=\"Original Split\"\n",
    "    )\n",
    "    \n",
    "    results2 = run_experiment(\n",
    "        df, \n",
    "        train_size=400, \n",
    "        val_size=400, \n",
    "        test_size=800,\n",
    "        experiment_name=\"New Split\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPARISON OF RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<30} {'Original (900/300/400)':<25} {'New (400/400/800)':<25}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Best k':<30} {results1['best_k']:<25} {results2['best_k']:<25}\")\n",
    "    print(f\"{'Validation Accuracy':<30} {results1['best_validation_accuracy']:<25.4f} {results2['best_validation_accuracy']:<25.4f}\")\n",
    "    print(f\"{'Validation Error':<30} {results1['validation_error']:<25.4f} {results2['validation_error']:<25.4f}\")\n",
    "    print(f\"{'Test Accuracy':<30} {results1['test_accuracy']:<25.4f} {results2['test_accuracy']:<25.4f}\")\n",
    "    print(f\"{'Generalisation Error':<30} {results1['generalisation_error']:<25.4f} {results2['generalisation_error']:<25.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYSIS AND EXPLANATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    k_values = list(range(1, 101))\n",
    "    plt.plot(k_values, results1['validation_errors'], 'b-', linewidth=1.5, \n",
    "             label=f'Original Split (900/300/400), Best k={results1[\"best_k\"]}')\n",
    "    plt.plot(k_values, results2['validation_errors'], 'r-', linewidth=1.5, \n",
    "             label=f'New Split (400/400/800), Best k={results2[\"best_k\"]}')\n",
    "    plt.xlabel('k (Number of Neighbours)', fontsize=12)\n",
    "    plt.ylabel('Validation Error', fontsize=12)\n",
    "    plt.title('Comparison of Validation Errors: Original vs New Split', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(0, 101, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('validation_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    print(\"\\nComparison plot saved as 'validation_comparison.png'\")\n",
    "    \n",
    "    return results1, results2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results1, results2 = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
