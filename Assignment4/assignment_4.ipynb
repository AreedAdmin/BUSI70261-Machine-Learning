{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: K-Means and Hierarchical Clustering\n",
    "Group K:\n",
    "Shehab Hassani 06071687\n",
    "Fares Gharbi 06052076\n",
    "Riddihma\n",
    "Leni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Z-Score Normalization\n",
    "\n",
    "Load the `customers.csv` dataset and apply Z-score normalization on the numerical features: **Age**, **Income**, and **Score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('customers.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalisation on numerical features: Age, Income, Score\n",
    "numerical_features = ['Age', 'Income', 'Score']\n",
    "scaler = StandardScaler()\n",
    "df_normalised = df.copy()\n",
    "df_normalised[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "print(\"Original data (first 5 rows):\")\n",
    "print(df[numerical_features].head())\n",
    "print(\"\\nNormalised data (first 5 rows):\")\n",
    "print(df_normalised[numerical_features].head())\n",
    "print(f\"\\nNormalised means (should be ~0): {df_normalised[numerical_features].mean().values}\")\n",
    "print(f\"Normalised stds  (should be ~1): {df_normalised[numerical_features].std().values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: K-Means with k = 2..10 and Best k Selection\n",
    "\n",
    "Run K-means for k = 2, 3, ..., 10 using the normalised features. Use the **Elbow Method** (inertia / WCSS) and **Silhouette Score** to determine the best k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalised = df_normalised[numerical_features].values\n",
    "\n",
    "# Run K-means for k = 2..10\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_normalised)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_normalised, labels))\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame({\n",
    "    'k': list(k_range),\n",
    "    'Inertia (WCSS)': inertias,\n",
    "    'Silhouette Score': silhouette_scores\n",
    "})\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Elbow Method and Silhouette Score\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow Method\n",
    "axes[0].plot(list(k_range), inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia (WCSS)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method', fontsize=14)\n",
    "axes[0].set_xticks(list(k_range))\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette Score\n",
    "axes[1].plot(list(k_range), silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score', fontsize=14)\n",
    "axes[1].set_xticks(list(k_range))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine the best k using the silhouette score\n",
    "best_k = list(k_range)[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nBest k (highest silhouette score): {best_k}\")\n",
    "print(f\"Silhouette score at k={best_k}: {max(silhouette_scores):.4f}\")\n",
    "print(\n",
    "    f\"\\nThe elbow method shows a noticeable bend, \"\n",
    "    f\"while the silhouette score is maximised at k={best_k}. \"\n",
    "    f\"We select k={best_k} as the optimal number of clusters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: 3D Cluster Visualisation with Best k\n",
    "\n",
    "Cluster the samples using K-means with the best k. Plot the clusters and centroids in 3D using the **denormalised** (original) axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-means with the best k\n",
    "kmeans_best = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans_best.fit_predict(X_normalised)\n",
    "\n",
    "# Denormalise the centroids back to original scale\n",
    "centroids_normalised = kmeans_best.cluster_centers_\n",
    "centroids_original = scaler.inverse_transform(centroids_normalised)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "# Print centroid information\n",
    "centroid_df = pd.DataFrame(\n",
    "    centroids_original,\n",
    "    columns=numerical_features,\n",
    "    index=[f'Cluster {i}' for i in range(best_k)]\n",
    ")\n",
    "print(f\"K-Means with k={best_k} — Cluster Centroids (original scale):\\n\")\n",
    "print(centroid_df.round(2))\n",
    "print(f\"\\nCluster sizes:\")\n",
    "print(df['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D scatter plot with denormalised axes\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Use a colormap for distinct cluster colours\n",
    "colours = plt.cm.tab10(np.linspace(0, 1, best_k))\n",
    "\n",
    "for i in range(best_k):\n",
    "    mask = cluster_labels == i\n",
    "    ax.scatter(\n",
    "        df.loc[mask, 'Age'],\n",
    "        df.loc[mask, 'Income'],\n",
    "        df.loc[mask, 'Score'],\n",
    "        c=[colours[i]],\n",
    "        label=f'Cluster {i}',\n",
    "        alpha=0.6,\n",
    "        s=40\n",
    "    )\n",
    "\n",
    "# Plot centroids\n",
    "ax.scatter(\n",
    "    centroids_original[:, 0],\n",
    "    centroids_original[:, 1],\n",
    "    centroids_original[:, 2],\n",
    "    c='black',\n",
    "    marker='X',\n",
    "    s=200,\n",
    "    edgecolors='white',\n",
    "    linewidths=1.5,\n",
    "    label='Centroids',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Age', fontsize=12)\n",
    "ax.set_ylabel('Income (k$)', fontsize=12)\n",
    "ax.set_zlabel('Spending Score', fontsize=12)\n",
    "ax.set_title(f'Customer Clusters (k={best_k}) — 3D View', fontsize=14)\n",
    "ax.legend(fontsize=10, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Customer Segments\n",
    "\n",
    "The K-means clustering reveals distinct customer segments based on their Age, Annual Income, and Spending Score. Typical segments that emerge include:\n",
    "\n",
    "- **High Income, High Spending Score**: Premium customers who earn well and spend generously — ideal targets for loyalty programmes and premium products.\n",
    "- **High Income, Low Spending Score**: Wealthy but cautious spenders — potential targets for targeted marketing to increase engagement.\n",
    "- **Low Income, High Spending Score**: Budget-conscious but enthusiastic shoppers — could benefit from discount programmes.\n",
    "- **Low Income, Low Spending Score**: Low-engagement customers — may require different strategies to attract.\n",
    "- **Average across all features**: The \"mainstream\" customer segment.\n",
    "\n",
    "The exact composition depends on the data; examine the centroid table above for precise segment characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Pairwise Feature K-Means\n",
    "\n",
    "Create three datasets with two out of three normalised features: **(Age, Income)**, **(Age, Score)**, **(Income, Score)**. Find the best k for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three feature pairs\n",
    "feature_pairs = [\n",
    "    ('Age', 'Income'),\n",
    "    ('Age', 'Score'),\n",
    "    ('Income', 'Score')\n",
    "]\n",
    "\n",
    "k_range_pairs = range(2, 11)\n",
    "pair_results = {}\n",
    "\n",
    "for pair in feature_pairs:\n",
    "    pair_name = f\"({pair[0]}, {pair[1]})\"\n",
    "    X_pair = df_normalised[list(pair)].values\n",
    "\n",
    "    inertias_pair = []\n",
    "    sil_scores_pair = []\n",
    "\n",
    "    for k in k_range_pairs:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        labels_pair = km.fit_predict(X_pair)\n",
    "        inertias_pair.append(km.inertia_)\n",
    "        sil_scores_pair.append(silhouette_score(X_pair, labels_pair))\n",
    "\n",
    "    best_k_pair = list(k_range_pairs)[np.argmax(sil_scores_pair)]\n",
    "    pair_results[pair_name] = {\n",
    "        'features': pair,\n",
    "        'inertias': inertias_pair,\n",
    "        'silhouette_scores': sil_scores_pair,\n",
    "        'best_k': best_k_pair,\n",
    "        'best_silhouette': max(sil_scores_pair)\n",
    "    }\n",
    "\n",
    "    print(f\"{pair_name}: Best k = {best_k_pair} \"\n",
    "          f\"(Silhouette = {max(sil_scores_pair):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Elbow and Silhouette for each feature pair\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 14))\n",
    "\n",
    "for idx, (pair_name, res) in enumerate(pair_results.items()):\n",
    "    # Elbow Method\n",
    "    axes[idx, 0].plot(\n",
    "        list(k_range_pairs), res['inertias'],\n",
    "        'bo-', linewidth=2, markersize=6\n",
    "    )\n",
    "    axes[idx, 0].axvline(\n",
    "        x=res['best_k'], color='red', linestyle='--',\n",
    "        alpha=0.7, label=f'Best k={res[\"best_k\"]}'\n",
    "    )\n",
    "    axes[idx, 0].set_xlabel('k')\n",
    "    axes[idx, 0].set_ylabel('Inertia (WCSS)')\n",
    "    axes[idx, 0].set_title(f'{pair_name} — Elbow Method')\n",
    "    axes[idx, 0].set_xticks(list(k_range_pairs))\n",
    "    axes[idx, 0].legend()\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Silhouette Score\n",
    "    axes[idx, 1].plot(\n",
    "        list(k_range_pairs), res['silhouette_scores'],\n",
    "        'ro-', linewidth=2, markersize=6\n",
    "    )\n",
    "    axes[idx, 1].axvline(\n",
    "        x=res['best_k'], color='blue', linestyle='--',\n",
    "        alpha=0.7, label=f'Best k={res[\"best_k\"]}'\n",
    "    )\n",
    "    axes[idx, 1].set_xlabel('k')\n",
    "    axes[idx, 1].set_ylabel('Silhouette Score')\n",
    "    axes[idx, 1].set_title(f'{pair_name} — Silhouette Score')\n",
    "    axes[idx, 1].set_xticks(list(k_range_pairs))\n",
    "    axes[idx, 1].legend()\n",
    "    axes[idx, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Gender-Based Cluster Analysis\n",
    "\n",
    "Plot the clusters for each feature pair and distinguish data points by Gender. Investigate whether gender reveals customer sub-segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster each feature pair with its best k and visualise with gender\n",
    "gender_markers = {'Male': 'o', 'Female': 's'}\n",
    "gender_colours = {'Male': 'steelblue', 'Female': 'coral'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (pair_name, res) in enumerate(pair_results.items()):\n",
    "    feat1, feat2 = res['features']\n",
    "    best_k_pair = res['best_k']\n",
    "    X_pair = df_normalised[[feat1, feat2]].values\n",
    "\n",
    "    # Fit K-means with the best k for this pair\n",
    "    km_pair = KMeans(n_clusters=best_k_pair, n_init=10, random_state=42)\n",
    "    pair_labels = km_pair.fit_predict(X_pair)\n",
    "\n",
    "    # Plot with gender differentiation\n",
    "    ax = axes[idx]\n",
    "    for gender in ['Male', 'Female']:\n",
    "        mask = df['Gender'] == gender\n",
    "        scatter = ax.scatter(\n",
    "            df.loc[mask, feat1],\n",
    "            df.loc[mask, feat2],\n",
    "            c=pair_labels[mask],\n",
    "            cmap='tab10',\n",
    "            marker=gender_markers[gender],\n",
    "            edgecolors='black',\n",
    "            linewidths=0.5,\n",
    "            alpha=0.7,\n",
    "            s=50,\n",
    "            label=gender,\n",
    "            vmin=0,\n",
    "            vmax=best_k_pair - 1\n",
    "        )\n",
    "\n",
    "    # Denormalise centroids for this pair\n",
    "    pair_scaler_mean = scaler.mean_[\n",
    "        [numerical_features.index(feat1),\n",
    "         numerical_features.index(feat2)]\n",
    "    ]\n",
    "    pair_scaler_std = scaler.scale_[\n",
    "        [numerical_features.index(feat1),\n",
    "         numerical_features.index(feat2)]\n",
    "    ]\n",
    "    centroids_pair_orig = (\n",
    "        km_pair.cluster_centers_ * pair_scaler_std + pair_scaler_mean\n",
    "    )\n",
    "    ax.scatter(\n",
    "        centroids_pair_orig[:, 0],\n",
    "        centroids_pair_orig[:, 1],\n",
    "        c='black', marker='X', s=200,\n",
    "        edgecolors='white', linewidths=1.5,\n",
    "        zorder=5, label='Centroids'\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(feat1, fontsize=12)\n",
    "    ax.set_ylabel(feat2, fontsize=12)\n",
    "    ax.set_title(f'{pair_name} — k={best_k_pair}', fontsize=13)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender distribution within each cluster (per feature pair)\n",
    "for pair_name, res in pair_results.items():\n",
    "    feat1, feat2 = res['features']\n",
    "    X_pair = df_normalised[[feat1, feat2]].values\n",
    "    km_pair = KMeans(\n",
    "        n_clusters=res['best_k'], n_init=10, random_state=42\n",
    "    )\n",
    "    pair_labels = km_pair.fit_predict(X_pair)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Feature pair: {pair_name} (k={res['best_k']})\")\n",
    "    print('='*50)\n",
    "    cross_tab = pd.crosstab(\n",
    "        pair_labels, df['Gender'],\n",
    "        margins=True, margins_name='Total'\n",
    "    )\n",
    "    cross_tab.index.name = 'Cluster'\n",
    "    print(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Gender and Sub-Segments\n",
    "\n",
    "By overlaying Gender on the 2D cluster plots, we can observe:\n",
    "\n",
    "1. **Gender distribution across clusters**: The cross-tabulation tables above show how Male and Female customers are distributed within each cluster. If gender is roughly uniformly distributed across clusters, it suggests that gender does not strongly define sub-segments beyond what Age, Income, and Score already capture.\n",
    "\n",
    "2. **Sub-segments**: Looking at the scatter plots, if certain clusters show a clear dominance of one gender, this hints at gender-based sub-segments (e.g., younger females with high spending scores, or older males with moderate income).\n",
    "\n",
    "3. **Should Gender be included in K-Means?**\n",
    "   - If gender shows a roughly equal split across all clusters, including it as a binary variable would add little value and may introduce noise due to the scale mismatch between a binary variable and continuous normalised features.\n",
    "   - If there is a clear gender imbalance in specific clusters, encoding Gender as a binary variable (0/1) could help K-means detect finer sub-segments. However, one must be cautious about the weight a binary variable carries relative to continuous features.\n",
    "   - In general, for this dataset, Gender tends to be fairly evenly distributed across clusters, suggesting it does **not** add significant discriminative power to the K-means algorithm. The existing numerical features already capture the main customer segments effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Hierarchical Clustering on the Noisy Dataset\n",
    "\n",
    "Load `customers_noisy.csv`, which contains the four original features (Gender is now binary) plus four noisy features. Perform hierarchical clustering on all features and plot the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the noisy dataset\n",
    "df_noisy = pd.read_csv('customers_noisy.csv')\n",
    "print(f\"Dataset shape: {df_noisy.shape}\")\n",
    "print(f\"Columns: {list(df_noisy.columns)}\")\n",
    "df_noisy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalise all features (Gender is already binary, but we\n",
    "# normalise all to ensure equal weighting in distance computation)\n",
    "noisy_features = df_noisy.columns.tolist()\n",
    "scaler_noisy = StandardScaler()\n",
    "X_noisy_normalised = scaler_noisy.fit_transform(df_noisy[noisy_features])\n",
    "\n",
    "print(\"Normalised noisy dataset (first 5 rows):\")\n",
    "print(pd.DataFrame(\n",
    "    X_noisy_normalised,\n",
    "    columns=noisy_features\n",
    ").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical (agglomerative) clustering using Ward's method\n",
    "Z = linkage(X_noisy_normalised, method='ward', metric='euclidean')\n",
    "\n",
    "# Plot the dendrogram\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=9,\n",
    "    show_contracted=True,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Sample Index (or cluster size)', fontsize=12)\n",
    "ax.set_ylabel('Distance (Ward)', fontsize=12)\n",
    "ax.set_title(\n",
    "    'Hierarchical Clustering Dendrogram — Noisy Dataset (All Features)',\n",
    "    fontsize=14\n",
    ")\n",
    "ax.axhline(y=50, color='r', linestyle='--', alpha=0.7,\n",
    "           label='Possible cut threshold')\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dendrogram (no truncation) for detailed view\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=6,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Sample Index', fontsize=12)\n",
    "ax.set_ylabel('Distance (Ward)', fontsize=12)\n",
    "ax.set_title(\n",
    "    'Full Dendrogram — Noisy Dataset (All Features)',\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare: hierarchical clustering on original features only\n",
    "original_cols = ['Gender', 'Age', 'Income', 'Score']\n",
    "X_orig_only = scaler_noisy.fit_transform(df_noisy[original_cols])\n",
    "Z_orig = linkage(X_orig_only, method='ward', metric='euclidean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    Z_orig,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=9,\n",
    "    show_contracted=True,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Sample Index (or cluster size)', fontsize=12)\n",
    "ax.set_ylabel('Distance (Ward)', fontsize=12)\n",
    "ax.set_title(\n",
    "    'Hierarchical Clustering Dendrogram — Original Features Only '\n",
    "    '(no noisy features)',\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Hierarchical Clustering on Noisy Data\n",
    "\n",
    "**Observations from the dendrogram:**\n",
    "\n",
    "1. **Effect of noisy features**: The four additional noisy features (Noisy1–Noisy4) introduce random variation that obscures the true cluster structure. When clustering on all 8 features, the dendrogram tends to show a less clear hierarchical structure — the merge distances become more uniform, and it is harder to identify a natural number of clusters.\n",
    "\n",
    "2. **Comparison with original features**: When we cluster using only the 4 original features (Gender, Age, Income, Score), the dendrogram shows more distinct cluster separations with larger gaps at key merge points, making it easier to identify a reasonable number of clusters (typically around 5, consistent with our K-means analysis).\n",
    "\n",
    "3. **Impact of noise**: The noisy features dilute the signal from the meaningful features. In high-dimensional noisy spaces, the distances between all pairs of points become more similar (the \"curse of dimensionality\"), making it harder for hierarchical clustering to find well-separated groups.\n",
    "\n",
    "4. **Practical takeaway**: Before applying clustering, it is important to perform feature selection or dimensionality reduction (e.g., PCA) to remove irrelevant or noisy features. Including noise in the distance computation degrades clustering quality and leads to less interpretable results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
