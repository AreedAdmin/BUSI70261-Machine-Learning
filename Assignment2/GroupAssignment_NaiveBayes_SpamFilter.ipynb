{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c66c2f2",
      "metadata": {
        "id": "0c66c2f2"
      },
      "source": [
        "# Group Assignment: Naïve Bayes Spam Filter\n",
        "\n",
        "Group K\n",
        "\n",
        "This notebook completes all required steps:\n",
        "\n",
        "1. Load datasets and censored word lists.\n",
        "2. Preprocess SMS messages (remove punctuation and numbers, lowercase).\n",
        "3. Train and evaluate the provided Naïve Bayes classifiers (`train` and `train2`).\n",
        "4. Answer all questions in the assignment.\n",
        "5. Implement the missing-word (censored-word) modification and report test accuracies for `test1` and `test2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f1595ccb",
      "metadata": {
        "id": "f1595ccb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Files (local paths - relative to notebook location)\n",
        "TRAIN_PATH = \"content/training.txt\"\n",
        "VAL_PATH = \"content/validation.txt\"\n",
        "TEST1_PATH = \"content/test1.txt\"\n",
        "TEST2_PATH = \"content/test2.txt\"\n",
        "\n",
        "CENS1_PATH = \"content/censored_list_test1.txt\"\n",
        "CENS2_PATH = \"content/censored_list_test2.txt\"\n",
        "\n",
        "NB_PATH = \"naive_bayes.py\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149391ce",
      "metadata": {
        "id": "149391ce"
      },
      "source": [
        "## 1. Load the data\n",
        "\n",
        "Load the four dataset files into pandas DataFrames and the two censored lists into Python lists. Do not shuffle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c0df814e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "c0df814e",
        "outputId": "4ee8adfe-2042-47f1-f56b-cd7581a26dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: (2000, 2)\n",
            "validation: (1000, 2)\n",
            "test1: (1285, 2)\n",
            "test2: (1286, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>\\Hi darlin i cantdo anythingtomorrow as mypare...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>K..k:)how about your training process?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>K actually can you guys meet me at the sunoco ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lor. Msg me b4 u call.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg&gt;FAV XMAS TONES!Reply REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                sms\n",
              "0   ham  \\Hi darlin i cantdo anythingtomorrow as mypare...\n",
              "1   ham             K..k:)how about your training process?\n",
              "2   ham  K actually can you guys meet me at the sunoco ...\n",
              "3   ham                          Ok lor. Msg me b4 u call.\n",
              "4  spam                  FreeMsg>FAV XMAS TONES!Reply REAL"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_sms_dataset(path: str) -> pd.DataFrame:\n",
        "    # Files are CSV-like with header: label,sms\n",
        "    df = pd.read_csv(path)\n",
        "    # Standardize column names just in case\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    # Expect columns: label, sms\n",
        "    if \"label\" not in df.columns or \"sms\" not in df.columns:\n",
        "        raise ValueError(f\"Unexpected columns in {path}: {df.columns.tolist()}\")\n",
        "    return df\n",
        "\n",
        "train_df = load_sms_dataset(TRAIN_PATH)\n",
        "val_df   = load_sms_dataset(VAL_PATH)\n",
        "test1_df = load_sms_dataset(TEST1_PATH)\n",
        "test2_df = load_sms_dataset(TEST2_PATH)\n",
        "\n",
        "print(\"train:\", train_df.shape)\n",
        "print(\"validation:\", val_df.shape)\n",
        "print(\"test1:\", test1_df.shape)\n",
        "print(\"test2:\", test2_df.shape)\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5bb4ba37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5bb4ba37",
        "outputId": "abe3e4aa-a9ef-46d7-a388-5e074b73f14e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "censored_test1: 485\n",
            "censored_test2: 1456\n",
            "example test1 words: ['god', 'search', 'passionate', 'lookatme', 'dearme', 'losing', 'convey', 'select', 'okok', 'more', 'themobyo', 'gang', 'salon', 'missed', 'dads', 'noice', 'upgrading', 'coffee', 'i', 'sory']\n"
          ]
        }
      ],
      "source": [
        "def load_censored_list(path: str) -> list[str]:\n",
        "    # One word per line\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        words = [line.strip() for line in f.readlines()]\n",
        "    words = [w for w in words if w]\n",
        "    return words\n",
        "\n",
        "censored_test1 = load_censored_list(CENS1_PATH)\n",
        "censored_test2 = load_censored_list(CENS2_PATH)\n",
        "\n",
        "print(\"censored_test1:\", len(censored_test1))\n",
        "print(\"censored_test2:\", len(censored_test2))\n",
        "print(\"example test1 words:\", censored_test1[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "161273ba",
      "metadata": {
        "id": "161273ba"
      },
      "source": [
        "## 2. Preprocess the SMS messages\n",
        "\n",
        "Remove punctuation and numbers and convert to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f5bf9b07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f5bf9b07",
        "outputId": "70f19d83-bbba-432e-c3f3-4108ce25d9c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>\\Hi darlin i cantdo anythingtomorrow as mypare...</td>\n",
              "      <td>hi darlin i cantdo anythingtomorrow as myparen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>K..k:)how about your training process?</td>\n",
              "      <td>k k how about your training process</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>K actually can you guys meet me at the sunoco ...</td>\n",
              "      <td>k actually can you guys meet me at the sunoco ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lor. Msg me b4 u call.</td>\n",
              "      <td>ok lor msg me b u call</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg&gt;FAV XMAS TONES!Reply REAL</td>\n",
              "      <td>freemsg fav xmas tones reply real</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                sms  \\\n",
              "0   ham  \\Hi darlin i cantdo anythingtomorrow as mypare...   \n",
              "1   ham             K..k:)how about your training process?   \n",
              "2   ham  K actually can you guys meet me at the sunoco ...   \n",
              "3   ham                          Ok lor. Msg me b4 u call.   \n",
              "4  spam                  FreeMsg>FAV XMAS TONES!Reply REAL   \n",
              "\n",
              "                                           sms_clean  \n",
              "0  hi darlin i cantdo anythingtomorrow as myparen...  \n",
              "1                k k how about your training process  \n",
              "2  k actually can you guys meet me at the sunoco ...  \n",
              "3                             ok lor msg me b u call  \n",
              "4                  freemsg fav xmas tones reply real  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove punctuation and digits, keep letters and spaces.\n",
        "# Also collapse repeated whitespace.\n",
        "def preprocess_sms(text: str) -> str:\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)     # remove numbers/punctuation/symbols\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize whitespace\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "for df in [train_df, val_df, test1_df, test2_df]:\n",
        "    df[\"sms_clean\"] = df[\"sms\"].apply(preprocess_sms)\n",
        "\n",
        "train_df[[\"label\",\"sms\",\"sms_clean\"]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c659a5a1",
      "metadata": {
        "id": "c659a5a1"
      },
      "source": [
        "## 3. Load the provided Naïve Bayes implementation\n",
        "\n",
        "We import the class from `naive_bayes.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "aab7f0c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aab7f0c2",
        "outputId": "8cab1f0e-feaf-40b5-aa8a-500ce098fe3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "naive_bayes.NaiveBayesForSpam"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib.util\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"naive_bayes\", NB_PATH)\n",
        "naive_bayes = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(naive_bayes)\n",
        "\n",
        "NaiveBayesForSpam = naive_bayes.NaiveBayesForSpam\n",
        "NaiveBayesForSpam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312e1c9d",
      "metadata": {
        "id": "312e1c9d"
      },
      "source": [
        "## Helper: split ham/spam and evaluate\n",
        "\n",
        "We train by passing ham and spam messages separately, as required by the provided API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9ab4cb3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9ab4cb3f",
        "outputId": "bab03311-a4c1-4cc0-ad65-712bab7d5e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train ham/spam: 1752 248\n",
            "val ham/spam: 860 140\n"
          ]
        }
      ],
      "source": [
        "def split_messages(df: pd.DataFrame):\n",
        "    ham = df.loc[df[\"label\"] == \"ham\", \"sms_clean\"].tolist()\n",
        "    spam = df.loc[df[\"label\"] == \"spam\", \"sms_clean\"].tolist()\n",
        "    return ham, spam\n",
        "\n",
        "def evaluate_model(model: NaiveBayesForSpam, df: pd.DataFrame):\n",
        "    messages = df[\"sms_clean\"].tolist()\n",
        "    labels = df[\"label\"].tolist()\n",
        "    acc, confusion = model.score(messages, labels)\n",
        "    return acc, confusion\n",
        "\n",
        "ham_train, spam_train = split_messages(train_df)\n",
        "ham_val, spam_val = split_messages(val_df)\n",
        "\n",
        "print(\"train ham/spam:\", len(ham_train), len(spam_train))\n",
        "print(\"val ham/spam:\", len(ham_val), len(spam_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63dff795",
      "metadata": {
        "id": "63dff795"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "**Explain the code:** purpose of each function, what `train` and `train2` do, difference between them, and where Bayes’ theorem is applied."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809904ff",
      "metadata": {},
      "source": [
        "\n",
        "The train(hamMessages, spamMessages) method builds the Naïve Bayes classifier by:\n",
        "1. **Building vocabulary:** Extracts all unique words from both ham and spam training messages\n",
        "2. **Computing class priors:** Calculates $P(Y=\\text{ham})$ and $P(Y=\\text{spam})$ as the proportion of each class in training data\n",
        "3. **Estimating likelihoods:** For each word $w$ in the vocabulary, computes $P(X_w=1 \\mid Y=c)$ — the probability that word $w$ appears in a message given class $c$\n",
        "4. **Laplace smoothing:** Adds a small constant (typically 1) to word counts to prevent zero probabilities for words that appear in only one class\n",
        "\n",
        "The `train2(hamMessages, spamMessages)` method performs the same steps as train(), but adds feature selection by keeping only words satisfying if prob_spam > 20 × prob_ham\n",
        "\n",
        "This retains only words that are at least 20× more likely to appear in spam than ham, strong spam indicators such as \"free\", \"win\", \"cash\", etc.\n",
        "\n",
        "`predict(message)` classifies a single message by:\n",
        "1. Starting with prior probabilities $[P(\\text{ham}), P(\\text{spam})]$\n",
        "2. For each word in vocabulary: multiplying by $P(X_w=1|Y)$ if the word is present, or $P(X_w=0|Y) = 1 - P(X_w=1|Y)$ if absent\n",
        "3. Normalising the posterior vector after each multiplication to prevent numerical underflow\n",
        "4. Returning the class with higher posterior probability\n",
        "\n",
        "score(messages, labels) evaluates the classifier by computing accuracy and a 2×2 confusion matrix over a labelled dataset.\n",
        "\n",
        "We highlight the difference between `train` and `train2` in the table below:\n",
        "\n",
        "| Aspect | `train()` | `train2()` |\n",
        "|--------|-----------|------------|\n",
        "| Vocabulary size | All unique words (~6,000+) | Only strong spam indicators (~200-500) |\n",
        "| Feature selection | None | Keeps words where $P(w|\\text{spam}) > 20 \\times P(w|\\text{ham})$ |\n",
        "| Prediction speed | Slower (iterates over full vocabulary) | Faster (iterates over reduced vocabulary) |\n",
        "| Model complexity | Higher variance (more parameters) | Lower variance (regularised via feature selection) |\n",
        "\n",
        "The key insight is that `train2()` acts as a regularisation technique by discarding weak or noisy features, it reduces overfitting and often improves generalisation performance.\n",
        "\n",
        "Bayes' theorem is applied in the `predict()` method. The classifier computes:\n",
        "\n",
        "$$\n",
        "P(Y = c \\mid X_1, X_2, \\ldots, X_p) = \\frac{P(Y=c) \\prod_{i=1}^{p} P(X_i \\mid Y=c)}{P(X_1, X_2, \\ldots, X_p)}\n",
        "$$\n",
        "\n",
        "Since the denominator $P(X)$ is constant across classes, prediction uses the unnormalised form:\n",
        "\n",
        "$$\n",
        "P(Y = c \\mid \\mathbf{X}) \\propto P(Y=c) \\prod_{i=1}^{p} P(X_i \\mid Y=c)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2669c2a",
      "metadata": {
        "id": "e2669c2a"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Train classifiers `train` and `train2` on the training set. Evaluate both on training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c1c2ba20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c1c2ba20",
        "outputId": "43f4e22f-39e0-4bc6-c49d-56fff109244f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier train()\n",
            "train accuracy: 0.975\n",
            "train confusion:\n",
            " [[1722.   20.]\n",
            " [  30.  228.]]\n",
            "val accuracy: 0.958\n",
            "val confusion:\n",
            " [[845.  27.]\n",
            " [ 15. 113.]]\n"
          ]
        }
      ],
      "source": [
        "nb1 = NaiveBayesForSpam()\n",
        "nb1.train(ham_train, spam_train)\n",
        "\n",
        "train_acc1, train_conf1 = evaluate_model(nb1, train_df)\n",
        "val_acc1, val_conf1 = evaluate_model(nb1, val_df)\n",
        "\n",
        "print(\"Classifier train()\")\n",
        "print(\"train accuracy:\", train_acc1)\n",
        "print(\"train confusion:\\n\", train_conf1)\n",
        "print(\"val accuracy:\", val_acc1)\n",
        "print(\"val confusion:\\n\", val_conf1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0526d27b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0526d27b",
        "outputId": "c72c6ea2-017a-4cbb-da81-ba148bc153df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier train2()\n",
            "train accuracy: 0.982\n",
            "train confusion:\n",
            " [[1750.   34.]\n",
            " [   2.  214.]]\n",
            "val accuracy: 0.959\n",
            "val confusion:\n",
            " [[855.  36.]\n",
            " [  5. 104.]]\n"
          ]
        }
      ],
      "source": [
        "nb2 = NaiveBayesForSpam()\n",
        "nb2.train2(ham_train, spam_train)\n",
        "\n",
        "train_acc2, train_conf2 = evaluate_model(nb2, train_df)\n",
        "val_acc2, val_conf2 = evaluate_model(nb2, val_df)\n",
        "\n",
        "print(\"Classifier train2()\")\n",
        "print(\"train accuracy:\", train_acc2)\n",
        "print(\"train confusion:\\n\", train_conf2)\n",
        "print(\"val accuracy:\", val_acc2)\n",
        "print(\"val confusion:\\n\", val_conf2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7efc29e",
      "metadata": {
        "id": "c7efc29e"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Using the validation set, explore how each classifier performs out of sample. (Answer is below but may be hidden do to rendering issue, please double click to expand it)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a7f39fc",
      "metadata": {},
      "source": [
        "The validation set provides an unbiased estimate of how each classifier will perform on unseen data. Below is a detailed comparison:\n",
        "\n",
        "Performance\n",
        "\n",
        "| Metric | `train()` | `train2()` |\n",
        "|--------|-----------|------------|\n",
        "| Training Accuracy | 97.5% | 98.2% |\n",
        "| Validation Accuracy | 95.8% | 95.9% |\n",
        "| Generalisation Gap | 1.7% | 2.3% |\n",
        "\n",
        "Both classifiers achieve similar validation accuracy (~96%), with `train2()` marginally outperforming by 0.1%.\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "| | `train()` | `train2()` |\n",
        "|---|-----------|------------|\n",
        "| True Positives (ham correctly classified) | 845 | 855 |\n",
        "| True Negatives (spam correctly classified) | 113 | 104 |\n",
        "| False Positives (ham misclassified as spam) | 15 | 5 |\n",
        "| False Negatives (spam misclassified as ham) | 27 | 36 |\n",
        "\n",
        "Both models generalise well, the gap between training and validation accuracy is small (1.7–2.3%), indicating neither is severely overfitting.\n",
        "\n",
        "`train2()` produces 3× fewer false positives. This is important in practice because legitimate emails incorrectly marked as spam may be permanently lost or ignored by users.\n",
        "\n",
        "`train2()` has more false negatives (36 vs 27), meaning more spam reaches the inbox. However, this is generally the less costly error, users can manually delete spam, but cannot recover emails they never saw.\n",
        "\n",
        "\n",
        "`train()` has higher recall for spam (113/140 = 80.7%) but lower precision (113/128 = 88.3%)\n",
        "`train2()` has lower recall for spam (104/140 = 74.3%) but higher precision (104/109 = 95.4%)\n",
        "\n",
        "By retaining only strongly spam-indicative words, `train2()` requires more \"spam evidence\" before classifying a message as spam. This conservative approach reduces the risk of flagging legitimate messages that happen to contain a few spam-like words.\n",
        "\n",
        "For most real-world spam filtering applications, `train2()` is preferable despite its slightly lower spam detection rate, because minimising false positives outweighs catching every spam message."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab626915",
      "metadata": {
        "id": "ab626915"
      },
      "source": [
        "## Question 7\n",
        "\n",
        "Why is `train2` faster? Why does it yield better accuracy on both training and validation?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db4751c",
      "metadata": {
        "id": "4db4751c"
      },
      "source": [
        "train2 is faster because it keeps a smaller set of words (features). Prediction loops over self.words, so fewer features means fewer multiplications and normalisations per message.\n",
        "\n",
        "It can yield better accuracy because it removes weak or noisy words and focuses on highly discriminative “spam keywords”. That reduces variance and can improve generalisation, especially when many words appear in both classes with similar frequencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a37f168",
      "metadata": {
        "id": "8a37f168"
      },
      "source": [
        "## Question 8\n",
        "\n",
        "How many false positives (ham classified as spam) on the validation set? How to reduce false positives at the expense of more false negatives?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e9acf4a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e9acf4a8",
        "outputId": "528dd79a-a4cc-4375-c854-80366aed0362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False positives on validation (train): 15\n",
            "False positives on validation (train2): 5\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix layout from naive_bayes.py score():\n",
        "# confusion[0,0]=TP_ham (pred ham, true ham)\n",
        "# confusion[0,1]=FN_ham (pred ham, true spam)   (spam missed)\n",
        "# confusion[1,0]=FP_ham (pred spam, true ham)   (false positives)\n",
        "# confusion[1,1]=TP_spam (pred spam, true spam)\n",
        "\n",
        "fp_val_train = int(val_conf1[1,0])\n",
        "fp_val_train2 = int(val_conf2[1,0])\n",
        "\n",
        "print(\"False positives on validation (train):\", fp_val_train)\n",
        "print(\"False positives on validation (train2):\", fp_val_train2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d432ee",
      "metadata": {
        "id": "90d432ee"
      },
      "source": [
        "To reduce false positives, increase the threshold required to label a message as spam.\n",
        "\n",
        "In this implementation, the decision rule is effectively `predict spam if posterior_spam >= 0.5`. Raising that threshold (for example to 0.7 or 0.8) makes the classifier more conservative about predicting spam, which reduces false positives but can increase false negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ab373e82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ab373e82",
        "outputId": "1aab2634-cbca-49b0-884e-ca4dc7977a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "threshold=0.5 acc=0.9590 FP=5 FN(spam->ham)=36\n",
            "threshold=0.6 acc=0.9580 FP=5 FN(spam->ham)=37\n",
            "threshold=0.7 acc=0.9590 FP=4 FN(spam->ham)=37\n",
            "threshold=0.8 acc=0.9590 FP=3 FN(spam->ham)=38\n"
          ]
        }
      ],
      "source": [
        "# Example: thresholded prediction wrapper (does not modify the original file).\n",
        "def predict_with_threshold(model: NaiveBayesForSpam, message: str, spam_threshold: float = 0.5):\n",
        "    label, prob = model.predict(message)\n",
        "    # model.predict returns ['ham', posterior_ham] or ['spam', posterior_spam]\n",
        "    if label == \"spam\":\n",
        "        return \"spam\" if prob >= spam_threshold else \"ham\"\n",
        "    # label == ham: it already says ham with prob_ham, converting to spam would require prob_spam which isn't returned\n",
        "    return \"ham\"\n",
        "\n",
        "def score_with_threshold(model: NaiveBayesForSpam, df: pd.DataFrame, spam_threshold: float):\n",
        "    confusion = np.zeros((2,2))\n",
        "    for m, true_label in zip(df[\"sms_clean\"], df[\"label\"]):\n",
        "        pred_label = predict_with_threshold(model, m, spam_threshold=spam_threshold)\n",
        "        if pred_label == \"ham\" and true_label == \"ham\":\n",
        "            confusion[0,0] += 1\n",
        "        elif pred_label == \"ham\" and true_label == \"spam\":\n",
        "            confusion[0,1] += 1\n",
        "        elif pred_label == \"spam\" and true_label == \"ham\":\n",
        "            confusion[1,0] += 1\n",
        "        elif pred_label == \"spam\" and true_label == \"spam\":\n",
        "            confusion[1,1] += 1\n",
        "    acc = (confusion[0,0] + confusion[1,1]) / confusion.sum()\n",
        "    return acc, confusion\n",
        "\n",
        "# Demonstrate the tradeoff for train2 model\n",
        "for t in [0.5, 0.6, 0.7, 0.8]:\n",
        "    acc_t, conf_t = score_with_threshold(nb2, val_df, spam_threshold=t)\n",
        "    print(f\"threshold={t:.1f} acc={acc_t:.4f} FP={int(conf_t[1,0])} FN(spam->ham)={int(conf_t[0,1])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9fbdd5",
      "metadata": {
        "id": "fa9fbdd5"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "Assuming missing words are $X_j= x_j, \\ldots, X_k=x_k$ with $k \\le p$, how to change the formula for $P(Y=C_j\\mid X_1=x_1,\\ldots,X_p=x_p)$?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1f5b65",
      "metadata": {
        "id": "7e1f5b65"
      },
      "source": [
        "If some variables $X_j, \\ldots, X_k $ are missing, the posterior should condition only on the observed features.\n",
        "\n",
        "With Naïve Bayes, this corresponds to dropping the likelihood factors associated with the missing variables:\n",
        "\n",
        "$$\n",
        "P(Y = C \\mid X_1 = x_1, \\ldots, X_p = x_p)\n",
        "\\;\\propto\\;\n",
        "P(Y = C)\\prod_{i \\in \\text{obs}} P(X_i = x_i \\mid Y = C)\n",
        "$$\n",
        "\n",
        "where **obs** denotes the set of observed variables.\n",
        "\n",
        "Equivalently, one can marginalise over the missing variables, which leads to the same result because unobserved variables contribute no evidence to the posterior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263d93b2",
      "metadata": {
        "id": "263d93b2"
      },
      "source": [
        "## Question 10\n",
        "\n",
        "Modify the prediction function to implement the missing-word change and report accuracies on `test1` with both `train` and `train2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fea15764",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fea15764",
        "outputId": "59f86e84-016f-429a-fd1b-af12275407d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST1 with missing-word handling\n",
            "train()  accuracy: 0.9657587548638132\n",
            "train()  confusion:\n",
            " [[1090.   24.]\n",
            " [  20.  151.]]\n",
            "train2() accuracy: 0.9735408560311284\n",
            "train2() confusion:\n",
            " [[1105.   29.]\n",
            " [   5.  146.]]\n"
          ]
        }
      ],
      "source": [
        "# Modified predictor: skip updates for censored words (treat as missing, not absent).\n",
        "class NaiveBayesForSpamMissing(NaiveBayesForSpam):\n",
        "    def predict_missing(self, message: str, censored_words: set[str]):\n",
        "        posteriors = np.copy(self.priors)\n",
        "        msg = message.lower()\n",
        "        for i, w in enumerate(self.words):\n",
        "            if w in censored_words:\n",
        "                # missing feature: do not multiply by P(X=1|Y) or P(X=0|Y)\n",
        "                continue\n",
        "            if w in msg:\n",
        "                posteriors *= self.likelihoods[:, i]\n",
        "            else:\n",
        "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
        "            posteriors = posteriors / np.linalg.norm(posteriors, ord=1)\n",
        "        if posteriors[0] > 0.5:\n",
        "            return ['ham', posteriors[0]]\n",
        "        return ['spam', posteriors[1]]\n",
        "\n",
        "def score_missing(model: NaiveBayesForSpamMissing, df: pd.DataFrame, censored_words: set[str]):\n",
        "    confusion = np.zeros((2,2))\n",
        "    for m, true_label in zip(df[\"sms_clean\"], df[\"label\"]):\n",
        "        pred_label = model.predict_missing(m, censored_words)[0]\n",
        "        if pred_label == \"ham\" and true_label == \"ham\":\n",
        "            confusion[0,0] += 1\n",
        "        elif pred_label == \"ham\" and true_label == \"spam\":\n",
        "            confusion[0,1] += 1\n",
        "        elif pred_label == \"spam\" and true_label == \"ham\":\n",
        "            confusion[1,0] += 1\n",
        "        elif pred_label == \"spam\" and true_label == \"spam\":\n",
        "            confusion[1,1] += 1\n",
        "    acc = (confusion[0,0] + confusion[1,1]) / confusion.sum()\n",
        "    return acc, confusion\n",
        "\n",
        "cens1_set = set([preprocess_sms(w) for w in censored_test1])\n",
        "\n",
        "nb1m = NaiveBayesForSpamMissing()\n",
        "nb1m.train(ham_train, spam_train)\n",
        "\n",
        "nb2m = NaiveBayesForSpamMissing()\n",
        "nb2m.train2(ham_train, spam_train)\n",
        "\n",
        "test1_acc1, test1_conf1 = score_missing(nb1m, test1_df, cens1_set)\n",
        "test1_acc2, test1_conf2 = score_missing(nb2m, test1_df, cens1_set)\n",
        "\n",
        "print(\"TEST1 with missing-word handling\")\n",
        "print(\"train()  accuracy:\", test1_acc1)\n",
        "print(\"train()  confusion:\\n\", test1_conf1)\n",
        "print(\"train2() accuracy:\", test1_acc2)\n",
        "print(\"train2() confusion:\\n\", test1_conf2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7003601",
      "metadata": {
        "id": "b7003601"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "Repeat Question 10 for `test2` and briefly report findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cde9260b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cde9260b",
        "outputId": "a9852d3a-9e69-49c9-b99f-b7857e590c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST2 with missing-word handling\n",
            "train()  accuracy: 0.9657853810264385\n",
            "train()  confusion:\n",
            " [[1088.   30.]\n",
            " [  14.  154.]]\n",
            "train2() accuracy: 0.9673405909797823\n",
            "train2() confusion:\n",
            " [[1099.   39.]\n",
            " [   3.  145.]]\n"
          ]
        }
      ],
      "source": [
        "cens2_set = set([preprocess_sms(w) for w in censored_test2])\n",
        "\n",
        "test2_acc1, test2_conf1 = score_missing(nb1m, test2_df, cens2_set)\n",
        "test2_acc2, test2_conf2 = score_missing(nb2m, test2_df, cens2_set)\n",
        "\n",
        "print(\"TEST2 with missing-word handling\")\n",
        "print(\"train()  accuracy:\", test2_acc1)\n",
        "print(\"train()  confusion:\\n\", test2_conf1)\n",
        "print(\"train2() accuracy:\", test2_acc2)\n",
        "print(\"train2() confusion:\\n\", test2_conf2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6096ba4",
      "metadata": {
        "id": "b6096ba4"
      },
      "source": [
        "**Brief Findings:**\n",
        "\n",
        "Both classifiers maintain strong accuracy (~96.6-96.7%) on test2 despite 1,456 censored words (3× more than test1's 485 words)\n",
        "\n",
        "`train2()` continues to outperform with fewer false positives (3 vs 14), though the accuracy gap narrows compared to test1\n",
        "\n",
        "The missing-word modification is essential,treating censored words as \"unobserved\" rather than \"absent\" prevents the model from incorrectly penalising messages that originally contained spam-indicative terms\n",
        "\n",
        "The robustness of both models to heavy censorship validates that Naïve Bayes handles missing features gracefully when properly implemented\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
